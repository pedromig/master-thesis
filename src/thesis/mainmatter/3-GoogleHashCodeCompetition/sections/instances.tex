In the competition context, in combination with the problem statements, test
case instances are provided to participants with the primary aim of providing a
mechanism for scoring teams, thus quantitatively assessing the efficacy of their
strategies. These instances are carefully generated to conform to the stipulated
limits and constraints inherent to the challenge, as described in upon in the
problem statement.

The initial instance, commonly denoted as the~\textquote{example}, is routinely
included within the problem statement for contestants' reference. This instance
is included in the problem statement for contestants' reference, but is not
solved optimally. Its purpose is to illustrate the input and output format for
the instance and solution. However, the example is intentionally designed with
small dimensions, making it approachable via exact brute force methodologies.

Subsequent instances are typically designed to push the boundaries of the problem.
These instances are intentionally large and design to discourage exact methods
and general heuristics, aiming to thoroughly examine various aspects of the
problem and avoid that (non-exact) greedy approaches find an optimal solution.
The ruggedness of their objective space introduces challenges for solvers,
potentially rendering some of them ineffective or even unusable within the
available time budget.

In the competition context, teams are allowed to provide unique solutions for
each instance, thus becoming a common practice among participants to conduct
thorough cross-instance analysis. This practice proves valuable in revealing
patterns that can offer insights into tackling the challenge with greater
efficiency. As such, participants have the flexibility to develop focused
strategies for each instance. This can in fact be interesting for the study of
general-purpose meta-heuristics, to understand whether they can achieve
comparable results to instance-specific approaches.

Finally, given the articulated problem statements and the transparent instance
generation process, participants can create customized test instances. This
capability proves valuable for debugging purposes in a competition setting and
further advocates these problems as interesting benchmarks
for~\acrshort{black-box-optimization}~\cite{bartz-beielstein2020benchmarking}.