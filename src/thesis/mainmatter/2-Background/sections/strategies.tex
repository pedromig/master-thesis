Combinatorial optimization literature extensively documents a series of methods
for solving multiple
problems~\cite{papadimitriou1998combinatorial,yu2010combinatorial,luke2013essentialsa}.
The approaches followed by these methods are diverse and typically defined by
factors such as the time complexity, the quality and the strategy for finding
solutions. Particularly, algorithms are often classified in the literature as
exact, approximation, or heuristic based on the quality of solutions.~Moreover,
(meta-)heuristic approaches are often described in terms of constructive and
local search procedures.

\subsection{Exact, Approximation and Heuristic Methods}
\label{subsec:approaches}

\emph{Exact} methods are designed to find the optimal solution for a given
problem. These typically involve an exhaustive enumeration and evaluation of
solutions. However, in large problem instances, this may prove to be
computationally infeasible.~In the context
of~\acrshort{combinatorial-optimization} problems, two general exact algorithms
are well-known and widely studied:~\textit{Branch and Bound} and~\textit{Dynamic
  Programming}~\cite{clausen1999branch,festa2014brief}.

These algorithms operate by iteratively breaking down a problem into smaller,
interconnected or standalone sub-problems. The solutions to these are then
combined to form the final solution. However, each algorithm employs distinct
techniques to enhance the exploration of the search space.

In Branch and Bound approaches, the strategy revolves around utilizing bounds to
restrict the search space. Specifically, the upper bound facilitates pruning the
search tree, effectively eliminating the need to explore solutions that are
undoubtedly worse with respect to the best solution found by the algorithm at a
particular stage. Similarly, the lower bound guarantees that solutions of
inferior quality are rejected during the search process. On the other hand,
Dynamic Programming approaches leverage the optimal substructure
property~\cite{festa2014brief}, thereby avoiding recomputing repeated
sub-problems.

\emph{Approximation} methods are designed to find solutions that are provably
guaranteed to be close to the optimal quality with respect to a given
approximation factor. In fact, approximation methods are often able to solve
problems in polynomial time and yield solutions of relatively high
quality~\cite{williamson2011design}.  However, it is important to note that
they require a mathematical proof of approximation that is specific to the
problem at hand. Notably, a significant amount of research exists in this field
concerning~\acrshort{combinatorial-optimization}
problems~\cite{johnson1974approximation}.

\emph{Heuristic} methods work by finding solutions according to a general
\textquote{rule of thumb}, the quality of which can be verified through
experimentation. These methods do not provide any guarantees of optimality, as
they are derived from intuition and their effectiveness is closely tied to the
characteristics of the problem at hand. Nevertheless, they are reliable means of
finding solutions in difficult~\acrshort{combinatorial-optimization} problems,
typically yielding good solutions in a short time frame when compared to exact
methods.

A~\acrshort{meta-heuristic} is as a high-level heuristic method,
as alluded by the word \textquote{meta}, which describes a concept as an
abstraction of other. In the literature, the definition of meta-heuristic varies
across different sources, resulting in a lack of consensus on a formal
description~\cite{osman1996metaheuristics,blum2003metaheuristics,festa2014brief,luke2013essentialsa}.
Nonetheless, one commonly accepted definition,
by~\citet{osman1996metaheuristics}, captures the essence
of~\acrshort{meta-heuristic} methods, which can be defined as iterative
generation processes that~\emph{
  \textquote{guide and intelligently combine subordinate heuristics for
    exploring and exploiting solutions in the search space}
}.~Moreover, most, if not all heuristic and meta-heuristic approaches are defined
in terms of constructive and local search approaches, which we describe next.

\subsection{Constructive Search}
\label{subsec:contructive-search}

\acrfull{constructive-search} is an approach for optimization where from an
empty or partial solution for a given problem a feasible complete solution is
constructed by iteratively adding components selected from the ground set. The
construction process is guided by a pre-established set of rules, which may be
heuristic in nature or informed by other relevant information,~\eg{}, objective
value and bounds.~These rules determine what components from the ground set can
be included in the solution at each iteration. The construction stops when no
more components can be added to the solution,~\ie{}, the solution is complete.
For clarification, a generic pseudocode for a~\acrshort{constructive-search}
procedure~\cite{marti2013multistart} is shown in~\Cref{algorithm:cs}.

\begin{algorithm}
  \input{mainmatter/2-Background/algorithms/cs.tex}
  \caption{\acrlong{constructive-search} Procedure}
  \label{algorithm:cs}
\end{algorithm}

It is important to observe that a constructive search approach hinges on a
strategy for selecting a component to add to a solution, represented
in~\Cref{algorithm:cs} through the~\texttt{SelectComponent} function.~Moreover,
various other problem-specific details come into play. These encompass
activities such as enumerating components~($\mathcal{C}$), creating an empty solution, adding
a component to a solution, and evaluating its feasibility.~Notably, these details
directly influence the \acrshort{constructive-search} procedure's ability to
construct good solutions. This highlights the importance of having a solid
problem \textit{model}, a concept we will delve into further in this thesis.

\subsection{Local Search}
\label{subsec:local-search}

\acrfull{local-search} approaches begin, with a feasible solution to a given
problem, and then make modifications by adding, removing, and swapping
components in order to improve it. The range of possible modifications defined
by the user define the neighborhood structure for a problem,
which~\acrshort{local-search} approaches attempt to
exploit.~The~\acrshort{local-search} approach terminates when no neighboring
solution is better (or equal),~\ie{}, when the current solution is a local
optimum. In the scope of this work, a transformation that can be applied to a
solution within in the context of a \acrshort{local-search} procedure will be
referred to as~\textit{local move}.

The primary objective of a~\acrshort{local-search} process is to improve a
solution in the direction of the local optimum. However, it is common in
a~\acrshort{local-search} approach to allow worsening a solution in order to
explore previously unseen regions of the search space. This action is commonly
referred to as a~\textit{perturbation}~\cite{lourenco2010iterateda}.
Furthermore,~\acrshort{local-search} is frequently applied in sequence to a
constructive search phase, where a~\acrshort{constructive-search} algorithm is
used to construct a good initial solution, which is then further improved
through a~\acrshort{local-search} approach.

A generic pseudocode for a local search procedure is outlined
in~\Cref{algorithm:ls}. It's crucial to clarify that within the pseudocode, the
\texttt{Step} function signifies the execution of a local move, while the
\texttt{Perturb} function represents an optional action introducing a
perturbation to the solution, if possible. Additionally, the enumeration of
possible local moves~($\mathcal{M}$) and the selection of a specific local move to incorporate
into the solution are abstracted through the \texttt{LocalMoves} and
\texttt{SelectLocalMove} functions, respectively. Note that, as
in~\acrshort{constructive-search}, the problem-specific choices made regarding
the implementation of each of these functions will significantly influence the
effectiveness of the~\acrshort{local-search} procedure.

\begin{algorithm}
  \input{mainmatter/2-Background/algorithms/ls.tex}
  \caption{\acrlong{local-search} Procedure}
  \label{algorithm:ls}
\end{algorithm}