Optimization, as defined by Papadimitriou and
Steiglitz~\cite{papadimitriou1998combinatorial}, is the task concerning the
search for the optimal configuration or set of parameters that maximizes or
minimizes a given objective function. In other words, optimizing involves
finding the~\textquote{best} solution to a given problem among a set of feasible
solutions. Formally, an optimization problem can be defined as follows:

\begin{definition}[Optimization Problem~\cite{papadimitriou1998combinatorial}]
  \label{def:optimization-problem}
  An optimization problem is a tuple $(\mathcal{S}, f)$, where
  $\mathcal{S}$ is a set containing all feasible solutions, and $f$ is an
  objective (cost) function, with a mapping such that:

  \begin{equation}
    \label{eq:optimization-problem}
    f \colon \mathcal{S} \longrightarrow \mathbb{R}
  \end{equation}

  That is, each solution $s \in \mathcal{S}$, is assigned a real value
  representing its quality, with the highest quality solution $s^{*} \in
    \mathcal{S}$ being referred to as the (globally) optimal solution.
\end{definition}

\begin{definition}[Globally Optimal Solution~\cite{hiriart-urruty1995conditions,papadimitriou1998combinatorial}]
  \label{def:global-optimum}
  Assuming, without loss of generality an optimization problem with a maximizing
  objective function a globally optimal solution $s^* \in \mathcal{S}$ is
  expressed by:

  \begin{equation}
    \forall s \in \mathcal{S} \colon f(s^{*}) \geq f(s)
  \end{equation}

\end{definition}

Since Google Hash Code problems~\cite{llc2023codingcompetitionsarchive}
have a single-objective maximizing objective function, we will only consider
maximization in this work. However, it is possible to reformulate problems with
a minimizing objective function for maximization~\cite{nocedal2006numerical}
using the identity:

\begin{equation}
  \label{eq:max2min}
  \max{-f(s)} = \min{f(s)}
\end{equation}

\subsection{Combinatorial Optimization}
\label{sec:combinatorial-optimization}

Optimization problems can be divided into~\textit{discrete}
and~\textit{continuous} based on the domain of their variables. In problems with
~\textit{discrete} variables, solutions are defined on a finite, possibly
countably infinite, set of values~\cite{papadimitriou1998combinatorial}. As for
problems with~\textit{continuous} variables, the solutions take on any value on
a continuous (infinite) subset of real numbers. Nevertheless, there are problems
that involve both~\textit{discrete} and~\textit{continuous} variables, commonly
denominated as mixed~\cite{nocedal2006numerical}.

\acrfull{combinatorial-optimization} problems are a subset of optimization
problems characterized by a discrete solution space that typically involves
different permutations, groupings, or orderings of objects that satisfy some
problem-specific criteria~\cite{papadimitriou1998combinatorial,vieira2009uma}.
As such, solutions for these problems are represented objects related to the
combinatorics~\eg{} integers, permutations sets and
graphs~\cite{blummetaheuristics,yu2010combinatorial}. Thus, regarding the
previous definition of an optimization problem,
a~\acrshort{combinatorial-optimization} can be formally defined as follows:

\begin{definition}[Combinatorial Optimization Problem~\cite{papadimitriou1998combinatorial}]
  \label{def:combinatorial-optimization-problem}
  A combinatorial optimization problem is an optimization
  problem~(\ref{def:optimization-problem}) where the set $\mathcal{S}$ of
  feasible solutions is finite or countably infinite.
\end{definition}

Typical examples of~\acrshort{combinatorial-optimization} problems include
network flow, matching, scheduling, shortest path and decision problems.
Notably, the~\acrfull{knapsack-problem} is a
well-known~\cite{cacchiani2022knapsack, yu2010combinatorial,festa2014brief}
example of a~\acrshort{combinatorial-optimization} problem where the goal is to
find the subset of items with the highest total profit that can fit in a
knapsack without exceeding its maximum capacity.

Due to the inherent discreteness of decision spaces
within~\acrshort{combinatorial-optimization} optimization problems, solutions
can be understood as compositions of objects (components) selected from a finite
set that including all elements capable of contributing to a solution.  This
set, commonly known as the~\textquote{ground set}, can be defined as shown:

\begin{definition}[Ground Set~\cite{outeiro2021application,festa2014brief,marti2013multistart}]
  \label{def:ground-set}
  The ground set $\mathcal{G}$ of a~\acrshort{combinatorial-optimization}
  problem is a finite set of containing all possible components for the problem.

  \begin{equation}
    \label{eq:ground-set}
    \mathcal{G} \colon \{c_{1}, c_{2}, c_{3}, \ldots, c_{i}\}
  \end{equation}
\end{definition}

Hence, within the context of~\acrshort{combinatorial-optimization} problems, a
feasible solution constitutes a subset of the ground set, denoted as $s \in
  \mathcal{S} \subseteq 2^{\mathcal{G}}$, where the included components satisfy
problem-specific constraints. Moreover, a partial solution is defined by its
former capacity to incorporate additional components from the ground set. In
contrast, a complete solution is incapable of accepting further components
without violating feasibility, as opposed to the former which can be infeasible.

To illustrate these concepts, let's consider the practical example of the
\acrshort{knapsack-problem}. In this context, the ground set is the set of all
the available items (components). As such, a feasible solution is one in which
items placed within the knapsack do not exceed its capacity limit. A partial
solution is one where the knapsack is not yet full and additional items can
still be accommodated. Notably, in this case, the partial solution is still
feasible. Finally, a complete solution is a feasible solution where no further
items can be added due to capacity constraints.

In essence, since \acrshort{combinatorial-optimization} problems involve
choosing a combination of objects any algorithm that is able to enumerate the
entirety of the solution space can be used to solve these problems. However,
finding an optimal solution can be difficult, and exhaustive search strategies
may not be able to solve many of these problems, which are often
NP-Hard~\cite{yu2010combinatorial,festa2014brief} and thus not approachable by
algorithms in a reasonable amount of time. In these cases, approximation,
heuristic and~\acrshort{meta-heuristic} methods present themselves as effective
alternatives to be considered.

\subsection{Global and Local Optimization}

% With regard to the search of solutions for optimization problems, there are two
% primary strategies: \acrfull{global-optimization} and
% \acrfull{local-optimization}.
% 
% \acrshort{global-optimization} refers to the process of identifying a global
% optimal solution (\ref{definition:global-optimum}) to a given problem,
% regardless of its location within the solution space. In contrast,
% \acrshort{local-optimization} focuses on finding the best solution among those
% that are proximate in some sense. The concept of proximity is related to the
% definition of a neighborhood, which for a given solution is specified by a
% particular neighborhood structure defined as follows:
% 
% \begin{definition}[Neighborhood Structure~\cite{papadimitriou1998combinatorial, blummetaheuristics}]
%   \label{definition:neighborhood-structure}
%   A neighborhood structure for an optimization problem with instances $(\mathcal{S}, f)$ is a mapping:
%   \begin{equation}
%     \label{equation:neighborhood-structure}
%     \mathcal{N} \colon \mathcal{S} \longrightarrow 2^{\mathcal{S}}
%   \end{equation}
%   Such that, a set of neighboring solutions $\mathcal{N}(\hat{s}) \subseteq
%     \mathcal{S}$ is assigned for each solution $\hat{s} \in \mathcal{S}$.
%   This referred to as the neighborhood of $\hat{s}$.
% \end{definition}
% 
% In general, the neighborhood structure refers to the set of rules that
% must be applied to a solution in order to generate all of its neighbors.
% Additionally, we consider the following definition of local optimal solution.
% 
% \begin{definition}[(Strictly) Local Optimal Solution~\cite{blummetaheuristics,nocedal2006numerical}]
%   \label{definition:local-optimum}
%   Assuming maximization without loss of generality, a solution $\hat{s}$ is
%   local optimal with respect to a given neighborhood structure $N(\hat{s})$
%   iff:
%   \begin{equation}
%     \label{equation:local-maximum}
%     \forall s \in \mathcal{N}(\hat{s}) \colon f(\hat{s}) \geq f(s)
%   \end{equation}
%   Furthermore, $\hat{s}$ is considered strictly global optimal iff
%   \begin{equation}
%     \label{equation:strict-local-maximum}
%     \forall s \in \mathcal{N}(\hat{s}) \setminus \{\hat{s}\} \colon f(\hat{s}) > f(s)
%   \end{equation}
% \end{definition}
% 
% As an illustrative example, consider the objective function $f(s)$ shown in
% figure~\ref{fig:maxima}. With respect to the definitions~\ref{definition:global-optimum}
% and~\ref{definition:local-optimum} $s^2$, $s^3$ are (strictly) local optimal.
% 
% \begin{figure}[h]
%   \centering
%   \input{../assets/plot/plot.tex}
%   \caption{Global and Local Optimal Solutions}
%   \label{fig:maxima}
% \end{figure}
% 
% In practice, the decision to use either a global or local optimization strategy
% is often influenced by factors such as the available time budget and the
% preferences of the decision maker. While global optimization aims to find the
% optimal solution to a problem, the search process may be time-consuming or, in
% some cases, computationally infeasible due to the size of the search space.
% On the other hand, local optimization, while lacking the optimality guarantees
% of global optimization, is able to quickly generate good solutions that may
% be acceptable to the decision maker. Nonetheless, the quality of the solutions
% may be poor due to the ruggedness of the objective function fitness landscape
% (many local optima). Ultimately, the performance of both methods is closely
% tied to the knowledge of problem-specific features.
% 
% In a setting such as the Hash Code competition, the problems presented are
% designed to resemble real-world situations. Additionally, due to the time
% constraints, it is not in the interest of the contestants to use global optimization
% methods, as they are unlikely to finish on more complex problem instances.
% Instead, a balance between global and local optimization is typically employed,
% with contestants utilizing the advantages of both methods. This approach
% involves establishing a baseline with global optimization methods that explore
% unseen regions of the search space for potentially good solutions, and then
% utilizing local optimization methods for exploiting solutions that have
% already been found.
% 
% \subsection{Black-Box and Glass-Box Optimization}
% 
% In the field of optimization, two settings are commonly recognized:
% \acrfull{black-box-optimization} and \acrfull{glass-box-optimization}.
% 
% In \acrshort{black-box-optimization} optimization settings there is no
% information about the landscape of the function being optimized, constraints
% defining the set of feasible solutions~\cite{alarie2021two} or the objective function
% is too complex to be approached from an analytical perspective. As such,
% algorithms for achieving solutions for these problems do so only by interacting
% with the problem through the evaluation of potential candidate
% solutions~\cite{doerr2020complexity}. Meta-Heuristics, as will be later detailed
% are examples of methods that follow this approach for finding/improving
% solutions. By contrast, in \acrfull{glass-box-optimization} optimization, also
% known as ``white box'' optimization, there is a good understanding of the
% problem instance being optimized and the objective function
% properties~\cite{doerr2020complexity}. Hence, the algorithms used may take
% advantage of more analytical properties of the problem since they are
% transparent to the optimizer.
% 
% For the purpose of clarification, with regard to the previously mentioned
% \acrshort{travelling-salesman-problem}, a black-box strategy would entail the
% utilization of a search heuristic such as simulated
% annealing~\cite{luke2013essentialsa} to obtain solutions. This is because the
% algorithm only necessitates knowledge of how to evaluate the quality of
% solutions through the objective function and not any specific information about
% the function being optimized. Alternatively, if the problem were to be
% formulated as an Integer Linear Programming
% problem~\cite{nocedal2006numerical,papadimitriou1998combinatorial}, it would
% become amenable to a white-box approach, as the objective function would be
% accessible, and additional information about the problem could be inferred from
% it and provided to the algorithm.
% 
% In the context of the Hash Code competition, contestants typically engage with a
% problem from a black-box perspective, as the underlying objective function of
% the problem is not disclosed, and/or is too complex to formalize. Additionally,
% the process of formalization would be time-consuming and, as a result, the usage
% of white-box methods post-formalization would not be justified, given that they
% may be computationally slower. Despite this, it is important to note that
% white-box methods should not be overlooked as they construct a model of the
% problem in order to solve it, which could be applied to a certain extent in the
% context of black-box approaches.