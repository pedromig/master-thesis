@article{alarie2021two,
  title = {Two Decades of Blackbox Optimization Applications},
  author = {Alarie, Stéphane and Audet, Charles and Gheribi, Aïmen E. and Kokkolaras, Michael and Le Digabel, Sébastien},
  date = {2021-01-01},
  journaltitle = {EURO Journal on Computational Optimization},
  shortjournal = {EURO Journal on Computational Optimization},
  volume = {9},
  pages = {100011},
  issn = {2192-4406},
  doi = {10.1016/j.ejco.2021.100011},
  url = {https://www.sciencedirect.com/science/article/pii/S2192440621001386},
  urldate = {2023-01-10},
  abstract = {This article reviews blackbox optimization applications of direct search optimization methods over the past twenty years. Emphasis is placed on the Mesh Adaptive Direct Search (Mads) derivative-free optimization algorithm. The main focus is on applications in three specific fields: energy, materials science, and computational engineering design. Nevertheless, other applications in science and engineering, including patents, are also considered. The breadth of applications demonstrates the versatility of Mads and highlights the evolution of its accompanying software NOMAD as a standard tool for blackbox optimization.},
  langid = {english},
  keywords = {Applications,Blackbox optimization,Derivative-free optimization,Mesh adaptive direct search},
  file = {/home/pedro/Documents/doc/zotero/Alarie et al_2021_Two decades of blackbox optimization applications.pdf;/home/pedro/Zotero/storage/XG2UYSI9/S2192440621001386.html}
}

@book{bartz-beielstein2020benchmarking,
  title = {Benchmarking in {{Optimization}}: {{Best Practice}} and {{Open Issues}}},
  shorttitle = {Benchmarking in {{Optimization}}},
  author = {Bartz-Beielstein, Thomas and Doerr, Carola and Bossek, Jakob and Chandrasekaran, Sowmya and Eftimov, Tome and Fischbach, Andreas and Kerschke, Pascal and López-Ibáñez, Manuel and Malan, Katherine and Moore, Jason and Naujoks, Boris and Orzechowski, Patryk and Volz, Vanessa and Wagner, Markus and Weise, Thomas},
  date = {2020-07-07},
  abstract = {This survey compiles ideas and recommendations from more than a dozen researchers with different backgrounds and from different institutes around the world. Promoting best practice in benchmarking is its main goal. The article discusses eight essential topics in benchmarking: clearly stated goals, well-specified problems, suitable algorithms, adequate performance measures, thoughtful analysis, effective and efficient designs, comprehensible presentations, and guaranteed reproducibility. The final goal is to provide well-accepted guidelines (rules) that might be useful for authors and reviewers. As benchmarking in optimization is an active and evolving field of research this manuscript is meant to co-evolve over time by means of periodic updates.},
  file = {/home/pedro/Documents/doc/zotero/Bartz-Beielstein et al_2020_Benchmarking in Optimization.pdf}
}

@article{blum2003metaheuristics,
  title = {Metaheuristics in Combinatorial Optimization},
  author = {Blum, Christian},
  date = {2003},
  journaltitle = {ACM Computing Surveys},
  volume = {35},
  number = {3},
  langid = {english},
  file = {/home/pedro/Zotero/storage/FVMR32EN/Blum - Metaheuristics in combinatorial optimization.pdf}
}

@article{cacchiani2022knapsack,
  title = {Knapsack Problems — {{An}} Overview of Recent Advances. {{Part I}}: {{Single}} Knapsack Problems},
  shorttitle = {Knapsack Problems — {{An}} Overview of Recent Advances. {{Part I}}},
  author = {Cacchiani, Valentina and Iori, Manuel and Locatelli, Alberto and Martello, Silvano},
  date = {2022-07},
  journaltitle = {Computers \& Operations Research},
  shortjournal = {Computers \& Operations Research},
  volume = {143},
  pages = {105692},
  issn = {03050548},
  doi = {10.1016/j.cor.2021.105692},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054821003877},
  urldate = {2023-08-20},
  abstract = {After the seminal books by Martello and Toth (1990) and Kellerer, Pferschy, and Pisinger (2004), knapsack problems became a classical and rich research area in combinatorial optimization. The purpose of this survey, which is structured in two parts, is to cover the developments that appeared in this field after the publication of the latter volume. Part I is devoted to problems whose goal is to optimally assign items to a single knapsack. Besides the classical knapsack problems (binary, subset sum, bounded, unbounded, change-making), we review problems with special constraints (setups, multiple-choice, conflicts, precedences, sharing, compartments) as well as relatively recent fields of investigation, like robust and bilevel problems. The subsequent Part II covers multiple, multidimensional, and quadratic knapsack problems, and includes a succinct treatment of online and multiobjective knapsack problems.},
  langid = {english},
  file = {/home/pedro/Zotero/storage/7IREH239/Cacchiani et al. - 2022 - Knapsack problems — An overview of recent advances.pdf}
}

@article{cahon2004paradiseoa,
  title = {{{ParadisEO}}: {{A Framework}} for the {{Reusable Design}} of {{Parallel}} and {{Distributed Metaheuristics}}},
  shorttitle = {{{ParadisEO}}},
  author = {Cahon, S. and Melab, N. and Talbi, E.-G.},
  date = {2004-05},
  journaltitle = {Journal of Heuristics},
  shortjournal = {Journal of Heuristics},
  volume = {10},
  number = {3},
  pages = {357--380},
  issn = {1381-1231},
  doi = {10.1023/B:HEUR.0000026900.92269.ec},
  url = {http://link.springer.com/10.1023/B:HEUR.0000026900.92269.ec},
  urldate = {2023-08-21},
  langid = {english},
  file = {/home/pedro/Documents/doc/zotero/Cahon et al_2004_ParadisEO2.pdf}
}

@article{clausen1999branch,
  title = {Branch and {{Bound Algorithms}} - {{Principles}} and {{Examples}}.},
  author = {Clausen, Jens},
  date = {1999},
  journaltitle = {Branch and Bound Algorithms - Principles and Examples.},
  langid = {english},
  file = {/home/pedro/Zotero/storage/I4DJQKWM/Clausen - Branch and Bound Algorithms - Principles and Examp.pdf}
}

@article{digaspero2003easylocal,
  title = {{{EasyLocal}}++: An Object-Oriented Framework for the Flexible Design of Local-Search Algorithms},
  shorttitle = {{{EasyLocal}}++},
  author = {Di Gaspero, Luca and Schaerf, Andrea},
  date = {2003-07-10},
  journaltitle = {Software—Practice \& Experience},
  shortjournal = {Softw. Pract. Exper.},
  volume = {33},
  number = {8},
  pages = {733--765},
  issn = {0038-0644},
  doi = {10.1002/spe.524},
  url = {https://doi.org/10.1002/spe.524},
  urldate = {2023-08-21},
  abstract = {Local search is a paradigm for search and optimization problems, which has recently evidenced to be very effective for a large number of combinatorial problems. Despite the increasing interest of the research community in this subject, there is still a lack of a widely-accepted software tools for local search.We propose EASYLOCAL++, an object-oriented framework for the design and the analysis of local-search algorithms. The abstract classes that compose the framework specify and implement the invariant part of the algorithm and are meant to be specialized by concrete classes that supply the problem-dependent part. The framework provides the full control structures of the algorithms, and the user has only to write the problem-specific code. Furthermore, the framework comes with some tools that simplify the analysis of the algorithms.The architecture of EASYLOCAL++ provides a principled modularization for the solution of combinatorial problems by local search and helps the user by deriving a neat conceptual scheme of the application. It also supports the design of combinations of basic techniques and/or neighborhood structures.The framework has been tested in some applicative domains and has proved to be flexible enough in the implementation of algorithms for the solution of various scheduling problems.},
  keywords = {(meta-)heuristics,algorithms design and implementation,analysis of algorithms,local search}
}

@incollection{doerr2020complexity,
  title = {Complexity {{Theory}} for {{Discrete Black-Box Optimization Heuristics}}},
  booktitle = {Theory of {{Evolutionary Computation}}: {{Recent Developments}} in {{Discrete Optimization}}},
  author = {Doerr, Carola},
  editor = {Doerr, Benjamin and Neumann, Frank},
  date = {2020},
  series = {Natural {{Computing Series}}},
  pages = {133--212},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-29414-4_3},
  url = {https://doi.org/10.1007/978-3-030-29414-4_3},
  urldate = {2023-01-10},
  abstract = {A predominant topic in the theory of evolutionary algorithms and, more generally, theory of randomized black-box optimization techniques is running-time analysis. Running-time analysis is aimed at understanding the performance of a given heuristic on a given problem by bounding the number of function evaluations that are needed by the heuristic to identify a solution of a desired quality. As in general algorithms theory, this running-time perspective is most useful when it is complemented by a meaningful complexity theory that studies the limits of algorithmic solutions.},
  isbn = {978-3-030-29414-4},
  langid = {english},
  file = {/home/pedro/Documents/doc/zotero/Doerr_2020_Complexity Theory for Discrete Black-Box Optimization Heuristics.pdf}
}

@article{dorigo2006anta,
  title = {Ant Colony Optimization},
  author = {Dorigo, Marco and Birattari, Mauro and Stutzle, Thomas},
  date = {2006-11},
  journaltitle = {IEEE Computational Intelligence Magazine},
  volume = {1},
  number = {4},
  pages = {28--39},
  issn = {1556-6048},
  doi = {10.1109/MCI.2006.329691},
  abstract = {Swarm intelligence is a relatively new approach to problem solving that takes inspiration from the social behaviors of insects and of other animals. In particular, ants have inspired a number of methods and techniques among which the most studied and the most successful is the general purpose optimization technique known as ant colony optimization. Ant colony optimization (ACO) takes inspiration from the foraging behavior of some ant species. These ants deposit pheromone on the ground in order to mark some favorable path that should be followed by other members of the colony. Ant colony optimization exploits a similar mechanism for solving optimization problems. From the early nineties, when the first ant colony optimization algorithm was proposed, ACO attracted the attention of increasing numbers of researchers and many successful applications are now available. Moreover, a substantial corpus of theoretical results is becoming available that provides useful guidelines to researchers and practitioners in further applications of ACO. The goal of this article is to introduce ant colony optimization and to survey its most notable applications},
  eventtitle = {{{IEEE Computational Intelligence Magazine}}},
  keywords = {Animals,Ant colony optimization,Bridges,Competitive intelligence,Computational and artificial intelligence,Computational intelligence,Fluctuations,Guidelines,Insects,Problem-solving},
  file = {/home/pedro/Documents/doc/zotero/Dorigo et al_2006_Ant colony optimization.pdf;/home/pedro/Zotero/storage/EIW5N24M/4129846.html}
}

@incollection{dorigo2010anta,
  title = {Ant {{Colony Optimization}}: {{Overview}} and {{Recent Advances}}},
  shorttitle = {Ant {{Colony Optimization}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Dorigo, Marco and Stützle, Thomas},
  editor = {Gendreau, Michel and Potvin, Jean-Yves},
  date = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {227--263},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-1665-5_8},
  url = {https://doi.org/10.1007/978-1-4419-1665-5_8},
  urldate = {2023-01-15},
  abstract = {Ant Colony Optimization (ACO) is a metaheuristic that is inspired by the pheromone trail laying and following behavior of some ant species. Artificial ants in ACO are stochastic solution construction procedures that build candidate solutions for the problem instance under concern by exploiting (artificial) pheromone information that is adapted based on the ants’ search experience and possibly available heuristic information. Since the proposal of the Ant System, the first ACO algorithm, many significant research results have been obtained. These contributions focused on the development of high-performing algorithmic variants, the development of a generic algorithmic framework for ACO algorithms, successful applications of ACO algorithms to a wide range of computationally hard problems, and the theoretical understanding of properties of ACO algorithms. This chapter reviews these developments and gives an overview of recent research trends in ACO.},
  isbn = {978-1-4419-1665-5},
  langid = {english},
  keywords = {Heuristic Information,Local Search Algorithm,Pheromone Trail,Solution Component,Travel Salesman Problem},
  file = {/home/pedro/Documents/doc/zotero/Dorigo_Stützle_2010_Ant Colony Optimization.pdf}
}

@article{durillo2011jmetal,
  title = {{{jMetal}}: {{A Java}} Framework for Multi-Objective Optimization},
  shorttitle = {{{jMetal}}},
  author = {Durillo, Juan J. and Nebro, Antonio J.},
  date = {2011-10-01},
  journaltitle = {Advances in Engineering Software},
  shortjournal = {Advances in Engineering Software},
  volume = {42},
  number = {10},
  pages = {760--771},
  issn = {0965-9978},
  doi = {10.1016/j.advengsoft.2011.05.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0965997811001219},
  urldate = {2023-08-21},
  abstract = {This paper describes jMetal, an object-oriented Java-based framework aimed at the development, experimentation, and study of metaheuristics for solving multi-objective optimization problems. jMetal includes a number of classic and modern state-of-the-art optimizers, a wide set of benchmark problems, and a set of well-known quality indicators to assess the performance of the algorithms. The framework also provides support to carry out full experimental studies, which can be configured and executed by using jMetal’s graphical interface. Other features include the automatic generation of statistical information of the obtained results, and taking advantage of the current availability of multi-core processors to speed-up the running time of the experiments. In this work, we include two case studies to illustrate the use of jMetal in both solving a problem with a metaheuristic and designing and performing an experimental study.},
  keywords = {Experimentation,Metaheuristics,Multi-objective optimization,Object-oriented architecture,Performance assessment support,Software tool},
  file = {/home/pedro/Zotero/storage/RGF6DCAU/S0965997811001219.html}
}

@article{eglese1990simulated,
  title = {Simulated Annealing: {{A}} Tool for Operational Research},
  shorttitle = {Simulated Annealing},
  author = {Eglese, R. W.},
  date = {1990-06-15},
  journaltitle = {European Journal of Operational Research},
  shortjournal = {European Journal of Operational Research},
  volume = {46},
  number = {3},
  pages = {271--281},
  issn = {0377-2217},
  doi = {10.1016/0377-2217(90)90001-R},
  url = {https://www.sciencedirect.com/science/article/pii/037722179090001R},
  urldate = {2023-08-24},
  abstract = {This paper describes the Simulated Annealing algorithm and the physical analogy on which it is based. Some significant theoretical results are presented before describing how the algorithm may be implemented and some of the choices facing the user of this method. An overview is given of the experience of experiments with SA and some suggestions are made for ways to improve the performance of the algorithm by modifying the ‘pure’ SA approach.},
  keywords = {heuristics,optimisation,Simulated annealing},
  file = {/home/pedro/Zotero/storage/G5C3BZUY/Eglese - 1990 - Simulated annealing A tool for operational resear.pdf;/home/pedro/Zotero/storage/TLISA3GH/037722179090001R.html}
}

@inproceedings{festa2014brief,
  title = {A Brief Introduction to Exact, Approximation, and Heuristic Algorithms for Solving Hard Combinatorial Optimization Problems},
  booktitle = {2014 16th {{International Conference}} on {{Transparent Optical Networks}} ({{ICTON}})},
  author = {Festa, P.},
  date = {2014-07},
  pages = {1--20},
  doi = {10.1109/ICTON.2014.6876285},
  abstract = {This paper presents a short (and not exhaustive) introduction to the most used exact, approximation, and metaheuristic algorithms for solving hard combinatorial optimization problems. After introducing the basics of exact approaches such as Branch \& Bound and Dynamic Programming, we focus on the basics of the most studied approximation techniques and of the most applied algorithms for finding good suboptimal solutions, including genetic algorithms, simulated annealing, tabu search, variable neighborhood search, greedy randomized adaptive search procedures (GRASP), path relinking, and scatter search.},
  eventtitle = {2014 16th {{International Conference}} on {{Transparent Optical Networks}} ({{ICTON}})},
  keywords = {Approximate solutions,Approximation algorithms,Approximation methods,Dynamic programming,Exact solution,Hard combinatorial optimization,Heuristic algorithms,Linear programming,Metaheuristics,Optimized production technology},
  file = {/home/pedro/Documents/doc/zotero/Festa_2014_A brief introduction to exact, approximation, and heuristic algorithms for.pdf;/home/pedro/Zotero/storage/8XJGU9EV/6876285.html}
}

@online{fonseca2021nasf4nio,
  title = {Nasf4nio},
  author = {Fonseca, Carlos M.},
  date = {2021-10-27T15:22:26Z},
  origdate = {2018-10-26T12:34:34Z},
  url = {https://github.com/cmfonseca/nasf4nio},
  urldate = {2023-01-15},
  abstract = {Not Another Software Framework for Nature-Inspired Optimization}
}

@incollection{gendreau2010tabua,
  title = {Tabu {{Search}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Gendreau, Michel and Potvin, Jean-Yves},
  editor = {Gendreau, Michel and Potvin, Jean-Yves},
  date = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {41--59},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-1665-5_2},
  url = {https://doi.org/10.1007/978-1-4419-1665-5_2},
  urldate = {2023-01-15},
  abstract = {This chapter presents the fundamental concepts of tabu search (TS) in a tutorial fashion. Special emphasis is put on showing the relationships with classical local search methods and on the basic elements of any TS heuristic, namely the definition of the search space, the neighborhood structure, and the search memory. Other sections cover other important concepts such as search intensification and diversification and provide references to significant work on TS. Recent advances in TS are also briefly discussed.},
  isbn = {978-1-4419-1665-5},
  langid = {english},
  keywords = {Neighborhood Structure,Search Space,Tabu List,Tabu Search,Tabu Search Heuristic},
  file = {/home/pedro/Documents/doc/zotero/Gendreau_Potvin_2010_Tabu Search.pdf}
}

@book{glover1999tabu,
  title = {Tabu Search {{I}}},
  author = {Glover, Fred and Laguna, Manuel},
  date = {1999-01-01},
  volume = {1},
  abstract = {This paper presents the fundamental principles underlying tabu search as a strategy for combinatorial optimization problems. Tabu search has achieved impressive practical successes in applications ranging from scheduling and computer channel balancing to cluster analysis and space planning, and more recently has demonstrated its value in treating classical problems such as the traveling salesman and graph coloring problems. Nevertheless, the approach is still in its infancy, and a good deal remains to be discovered about its most effective forms of implementation and about the range of problems for which it is best suited. This paper undertakes to present the major ideas and findings to date, and to indicate challenges for future research. Part I of this study indicates the basic principles, ranging from the short-term memory process at the core of the search to the intermediate and long term memory processes for intensifying and diversifying the search. Included are illustrative data structures for implementing the tabu conditions (and associated aspiration criteria) that underlie these processes. Part I concludes with a discussion of probabilistic tabu search and a summary of computational experience for a variety of applications. Part II of this study (to appear in a subsequent issue) examines more advanced considerations, applying the basic ideas to special settings and outlining a dynamic move structure to insure finiteness. Part II also describes tabu search methods for solving mixed integer programming problems and gives a brief summary of additional practical experience, including the use of tabu search to guide other types of processes, such as those of neural networks. INFORMS Journal on Computing, ISSN 1091-9856, was published as ORSA Journal on Computing from 1989 to 1995 under ISSN 0899-1499.},
  isbn = {978-0-7923-9965-0},
  file = {/home/pedro/Documents/doc/zotero/Glover_Laguna_1999_Tabu search I.pdf}
}

@online{googlellc2023codingcompetitionsarchive,
  title = {Coding-Competitions-Archive},
  author = {{Google LLC}},
  date = {2023-08-19T14:07:26Z},
  origdate = {2023-03-09T11:01:52Z},
  url = {https://github.com/google/coding-competitions-archive},
  urldate = {2023-08-20},
  abstract = {Google Coding Competitions problem archive},
  langid = {english}
}

@incollection{gutjahr2010stochastic,
  title = {Stochastic {{Search}} in {{Metaheuristics}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Gutjahr, Walter J.},
  editor = {Gendreau, Michel and Potvin, Jean-Yves},
  date = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {573--597},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-1665-5_19},
  url = {https://doi.org/10.1007/978-1-4419-1665-5_19},
  urldate = {2023-08-25},
  abstract = {Stochastic search is a key mechanism underlying many metaheuristics. The chapter starts with the presentation of a general framework algorithm in the form of a stochastic search process that contains a large variety of familiar metaheuristic techniques as special cases. Based on this unified view, questions concerning convergence and runtime are discussed on the level of a theoretical analysis. Concrete examples from diverse metaheuristic fields are given. In connection with runtime results, important topics as instance difficulty, phase transitions, parameter choice, No-Free-Lunch theorems, or fitness landscape analysis are addressed. Furthermore, a short sketch of the theory of black-box optimization is given, and generalizations of results to stochastic search under noise are outlined.},
  isbn = {978-1-4419-1665-5},
  langid = {english},
  keywords = {Fitness Landscape,Metaheuristic Algorithm,Particle Swarm Optimization,Simulated Annealing,Variable Neighborhood Search},
  file = {/home/pedro/Documents/doc/zotero/Gutjahr_2010_Stochastic Search in Metaheuristics.pdf}
}

@incollection{hiriart-urruty1995conditions,
  title = {Conditions for {{Global Optimality}}},
  booktitle = {Handbook of {{Global Optimization}}},
  author = {Hiriart-Urruty, J.-B.},
  editor = {Horst, Reiner and Pardalos, Panos M.},
  date = {1995},
  series = {Nonconvex {{Optimization}} and {{Its Applications}}},
  pages = {1--26},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4615-2025-2_1},
  url = {https://doi.org/10.1007/978-1-4615-2025-2_1},
  urldate = {2023-08-21},
  abstract = {Given an optimization problem we are interested in getting characterizations of its global solutions, i.e., necessary and sufficient conditions for a feasible point to be a global minimum (or maximum) of the objective function. We review here recent theoretical results in that direction, emphasizing those which have led to algorithms for global optimization.},
  isbn = {978-1-4615-2025-2},
  langid = {english},
  keywords = {Convex Function,Global Minimum,Global Optimality,Quadratic Form,Trust Region}
}

@article{johnson1974approximation,
  title = {Approximation Algorithms for Combinatorial Problems},
  author = {Johnson, David S.},
  date = {1974-12-01},
  journaltitle = {Journal of Computer and System Sciences},
  shortjournal = {Journal of Computer and System Sciences},
  volume = {9},
  number = {3},
  pages = {256--278},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(74)80044-9},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000074800449},
  urldate = {2023-01-13},
  abstract = {Simple, polynomial-time, heuristic algorithms for finding approximate solutions to various polynomial complete optimization problems are analyzed with respect to their worst case behavior, measured by the ratio of the worst solution value that can be chosen by the algorithm to the optimal value. For certain problems, such as a simple form of the kanpsack problem and an optimization problem based on satisfiability testing, there are algorithms for which this ratio is bounded by a constant, independent of the problem size. For a number of set covering problems, simple algorithms yield worst case ratios which can grow with the log of the problem size. And for the problem of finding the maximum clique in a graph, no algorithm has been found for which the ratio does not grow at least as fast as nε, where n is the problem size and ε{$>$}0 depends on the algorithm.},
  langid = {english},
  file = {/home/pedro/Documents/doc/zotero/Johnson_1974_Approximation algorithms for combinatorial problems.pdf;/home/pedro/Zotero/storage/Z2683GCR/S0022000074800449.html}
}

@article{kirkpatrick1983optimization,
  title = {Optimization by {{Simulated Annealing}}},
  author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  date = {1983-05-13},
  journaltitle = {Science},
  volume = {220},
  number = {4598},
  pages = {671--680},
  doi = {10.1126/science.220.4598.671},
  url = {https://www.science.org/doi/abs/10.1126/science.220.4598.671},
  urldate = {2023-01-15},
  abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
  file = {/home/pedro/Documents/doc/zotero/Kirkpatrick et al_1983_Optimization by Simulated Annealing.pdf}
}

@incollection{knuth2014art,
  title = {The {{Art}} of {{Computer Programming}}: {{Seminumerical Algorithms}}, {{Volume}} 2},
  author = {Knuth, Donald E.},
  date = {2014-05-06},
  pages = {17-19; 145},
  publisher = {{Addison-Wesley Professional}},
  abstract = {The bible of all fundamental algorithms and the work that taught many of today's software developers most of what they know about computer programming.   –Byte, September 1995   I can't begin to tell you how many pleasurable hours of study and recreation they have afforded me! I have pored over them in cars, restaurants, at work, at home... and even at a Little League game when my son wasn't in the line-up.   –Charles Long   If you think you're a really good programmer... read [Knuth's] Art of Computer Programming... You should definitely send me a resume if you can read the whole thing.   –Bill Gates   It's always a pleasure when a problem is hard enough that you have to get the Knuths off the shelf. I find that merely opening one has a very useful terrorizing effect on computers.   –Jonathan Laventhol   The second volume offers a complete introduction to the field of seminumerical algorithms, with separate chapters on random numbers and arithmetic. The book summarizes the major paradigms and basic theory of such algorithms, thereby providing a comprehensive interface between computer programming and numerical analysis. Particularly noteworthy in this third edition is Knuth's new treatment of random number generators, and his discussion of calculations with formal power series.  Ebook (PDF version) produced by Mathematical Sciences Publishers (MSP),http://msp.org},
  isbn = {978-0-321-63576-1},
  langid = {english},
  keywords = {Computers / Programming / General},
  file = {/home/pedro/Documents/doc/zotero/Knuth_2014_.pdf}
}

@incollection{lourenco2010iterateda,
  title = {Iterated {{Local Search}}: {{Framework}} and {{Applications}}},
  shorttitle = {Iterated {{Local Search}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Lourenço, Helena R. and Martin, Olivier C. and Stützle, Thomas},
  editor = {Gendreau, Michel and Potvin, Jean-Yves},
  date = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {363--397},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-1665-5_12},
  url = {https://doi.org/10.1007/978-1-4419-1665-5_12},
  urldate = {2023-01-15},
  abstract = {The key idea underlying iterated local search is to focus the search not on the full space of all candidate solutions but on the solutions that are returned by some underlying algorithm, typically a local search heuristic. The resulting search behavior can be characterized as iteratively building a chain of solutions of this embedded algorithm. The result is also a conceptually simple metaheuristic that nevertheless has led to state-of-the-art algorithms for many computationally hard problems. In fact, very good performance is often already obtained by rather straightforward implementations of the metaheuristic. In addition, the modular architecture of iterated local search makes it very suitable for an algorithm engineering approach where, progressively, the algorithms’ performance can be further optimized. Our purpose here is to give an accessible description of the underlying principles of iterated local search and a discussion of the main aspects that need to be taken into account for a successful application of it. In addition, we review the most important applications of this method and discuss its relationship to other metaheuristics.},
  isbn = {978-1-4419-1665-5},
  langid = {english},
  keywords = {Acceptance Criterion,Local Search,Local Search Algorithm,Travel Salesman Problem,Variable Neighborhood Search},
  file = {/home/pedro/Documents/doc/zotero/Lourenço et al_2010_Iterated Local Search2.pdf}
}

@book{luke2013essentialsa,
  title = {Essentials of {{Metaheuristics}}},
  author = {Luke, Sean},
  date = {2013},
  edition = {Second Edition},
  url = {https://cs.gmu.edu/~sean/book/metaheuristics/},
  urldate = {2023-01-11},
  file = {/home/pedro/Documents/doc/zotero/Luke_2013_Essentials of Metaheuristics.pdf;/home/pedro/Zotero/storage/Z8GUI84X/metaheuristics.html}
}

@article{martello1981bound,
  title = {A {{Bound}} and {{Bound}} Algorithm for the Zero-One Multiple Knapsack Problem},
  author = {Martello, Silvano and Toth, Paolo},
  date = {1981-11-01},
  journaltitle = {Discrete Applied Mathematics},
  shortjournal = {Discrete Applied Mathematics},
  series = {Special {{Copy}}},
  volume = {3},
  number = {4},
  pages = {275--288},
  issn = {0166-218X},
  doi = {10.1016/0166-218X(81)90005-6},
  url = {https://www.sciencedirect.com/science/article/pii/0166218X81900056},
  urldate = {2023-01-16},
  abstract = {By the term “Bound and Bound” we define a particular tree-search technique for the ILP, which, for a maximization problem, makes use of a lower bound to determine the branches to follow in the decision tree. This technique is applied to the solution of the Zero-One Multiple Knapsack Problem and an algorithm is derived; an illustrative example of the procedure is provided. We present extensive computational results showing that the method is capable of solving problems up to 4 knapsacks and 200 variables with running times considerably smaller than those of the most commonly utilized algorithms.},
  langid = {english},
  file = {/home/pedro/Documents/doc/zotero/Martello_Toth_1981_A Bound and Bound algorithm for the zero-one multiple knapsack problem.pdf;/home/pedro/Zotero/storage/WH7KBDLN/0166218X81900056.html}
}

@article{marti2013multistart,
  title = {Multi-Start Methods for Combinatorial Optimization},
  author = {Martí, Rafael and Resende, Mauricio G. C. and Ribeiro, Celso C.},
  date = {2013-04-01},
  journaltitle = {European Journal of Operational Research},
  shortjournal = {European Journal of Operational Research},
  volume = {226},
  number = {1},
  pages = {2},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2012.10.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0377221712007394},
  urldate = {2023-01-10},
  abstract = {Multi-start methods strategically sample the solution space of an optimization problem. The most successful of these methods have two phases that are alternated for a certain number of global iterations. The first phase generates a solution and the second seeks to improve the outcome. Each global iteration produces a solution that is typically a local optimum, and the best overall solution is the output of the algorithm. The interaction between the two phases creates a balance between search diversification (structural variation) and search intensification (improvement), to yield an effective means for generating high-quality solutions. This survey briefly sketches historical developments that have motivated the field, and then focuses on modern contributions that define the current state-of-the-art. We consider two categories of multi-start methods: memory-based and memoryless procedures. The former are based on identifying and recording specific types of information (attributes) to exploit in future constructions. The latter are based on order statistics of sampling and generate unconnected solutions. An interplay between the features of these two categories provides an inviting area for future exploration.},
  langid = {english},
  keywords = {Adaptive memory programming,GRASP,Metaheuristics,Multi-start methods},
  file = {/home/pedro/Documents/doc/zotero/Martí et al_2013_Multi-start methods for combinatorial optimization.pdf;/home/pedro/Zotero/storage/LRZN7P6K/S0377221712007394.html}
}

@incollection{nikolaev2010simulateda,
  title = {Simulated {{Annealing}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Nikolaev, Alexander G. and Jacobson, Sheldon H.},
  editor = {Gendreau, Michel and Potvin, Jean-Yves},
  date = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {1--39},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-1665-5_1},
  url = {https://doi.org/10.1007/978-1-4419-1665-5_1},
  urldate = {2023-01-15},
  abstract = {Simulated annealing is a well-studied local search metaheuristic used to address discrete and, to a lesser extent, continuous optimization problems. The key feature of simulated annealing is that it provides a mechanism to escape local optima by allowing hill-climbing moves (i.e., moves which worsen the objective function value) in hopes of finding a global optimum. A brief history of simulated annealing is presented, including a review of its application to discrete, continuous, and multi-objective optimization problems. Asymptotic convergence and finite-time performance theory for simulated annealing are reviewed. Other local search algorithms are discussed in terms of their relationship to simulated annealing. The chapter also presents practical guidelines for the implementation of simulated annealing in terms of cooling schedules, neighborhood functions, and appropriate applications.},
  isbn = {978-1-4419-1665-5},
  langid = {english},
  keywords = {Discrete Optimization Problem,Simulated Annealing,Simulated Annealing Algorithm,Tabu Search,Travel Salesman Problem},
  file = {/home/pedro/Documents/doc/zotero/Nikolaev_Jacobson_2010_Simulated Annealing.pdf}
}

@book{nocedal2006numerical,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  date = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {{Springer New York}},
  doi = {10.1007/978-0-387-40065-5},
  url = {http://link.springer.com/10.1007/978-0-387-40065-5},
  urldate = {2023-01-10},
  isbn = {978-0-387-30303-1},
  langid = {english},
  keywords = {algorithms,Calculus of Variations,linear optimization,nonlinear optimization,operations research,optimization,quadratic programming,Quasi-Newton method},
  file = {/home/pedro/Documents/doc/zotero/2006_Numerical Optimization.pdf}
}

@article{osman1996metaheuristics,
  title = {Metaheuristics: {{A}} Bibliography},
  shorttitle = {Metaheuristics},
  author = {Osman, Ibrahim H. and Laporte, Gilbert},
  date = {1996-10-01},
  journaltitle = {Annals of Operations Research},
  shortjournal = {Ann Oper Res},
  volume = {63},
  number = {5},
  pages = {511--623},
  issn = {1572-9338},
  doi = {10.1007/BF02125421},
  url = {https://doi.org/10.1007/BF02125421},
  urldate = {2023-01-14},
  abstract = {Metaheuristics are the most exciting development in approximate optimization techniques of the last two decades. They have had widespread successes in attacking a variety of difficult combinatorial optimization problems that arise in many practical areas. This bibliography provides a classification of a comprehensive list of 1380 references on the theory and application of metaheuristics. Metaheuristics include but are not limited to constraint logic programming; greedy random adaptive search procedures; natural evolutionary computation; neural networks; non-monotonic search strategies; space-search methods; simulated annealing; tabu search; threshold algorithms and their hybrids. References are presented in alphabetical order under a number of subheadings.},
  langid = {english},
  keywords = {Artificial intelligence,bibliography,combinatorial optimization,constraint logic programming,evolutionary computation,genetic algorithms,greedy random adaptive search procedure,heuristics,hybrids,local search,metaheuristics,neural networks,non-monotonic search strategies,problem-space method,simulated annealing,tabu search,threshold algorithms},
  file = {/home/pedro/Documents/doc/zotero/Osman_Laporte_1996_Metaheuristics.pdf}
}

@thesis{outeiro2021application,
  type = {Msc Thesis},
  title = {An {{Application Programming Interface}} for {{Constructive Search}}},
  author = {family=Outeiro, given=Samuel Barroca, prefix=do, useprefix=false},
  date = {2021-11-09},
  institution = {{University of Coimbra, Portugal}},
  url = {https://estudogeral.sib.uc.pt/handle/10316/98261},
  urldate = {2023-01-10},
  abstract = {A utilização prática de otimização está intimamente relacionada com a disponibilidade de ferramentas de software para a suportar. As abordagens atuais para otimização combinatória caem em duas categorias: modelos de caixa branca (ou transparente) tais como Programação Linear Inteira (ILP) e Programação por Restrições (CP), e modelos de problemas e algoritmos (tipicamente heurísticos) de caixa cinzenta/preta que são diretamente implementados em software. Estes últimos poderão ser mais flexíveis e fáceis de integrar em fluxos de trabalho existentes. Contudo, a falta de convenções para a modelação/implementação de problemas de otimização em software dificultam a sua adoção na prática. De facto, distintos frameworks de software de otimização tipicamente obrigam a que os problemas estejam implementados nessa framework. Para além disso, a maioria das frameworks focam-se principalmente em algoritmos de procura local ao invés de procura construtiva. Assim, surge uma oportunidade para o desenvolvimento de uma Interface de Programação de Aplicações (API) para problemas e algoritmos de procura construtiva.Esta API separa o (modelo do) problema do algoritmo que o resolve através da especificação de várias operações elementares e abstratas que os problemas precisam de implementar e que os algoritmos podem usar independentemente do problema. Tanto algoritmos exatos, como (meta)heurísticos são suportados, dos quais se destacam o Branch \& Bound, o Beam Search, o GRASP, os algoritmos de Ant Colony Optimization, entre outros.As consequências da abstração proposta para o desenvolvimento de novos algoritmos de procura construtiva são também discutidos neste trabalho. Em particular, é conduzido um estudo sobre o efeito dos modelos de problemas no desempenho de algoritmos. Este estudo sugere que um melhor modelo poderá potencialmente beneficiar todos os algoritmos que o usam.},
  langid = {english},
  annotation = {Accepted: 2022-02-02T23:10:29Z},
  file = {/home/pedro/Documents/doc/zotero/Outeiro_2021_An Application Programming Interface for Constructive Search.pdf}
}

@article{ow1988filtered,
  title = {Filtered Beam Search in Scheduling†},
  author = {Ow, Peng Si and Morton, Thomas E.},
  date = {1988-01-01},
  journaltitle = {International Journal of Production Research},
  volume = {26},
  number = {1},
  pages = {35--62},
  issn = {0020-7543},
  doi = {10.1080/00207548808947840},
  url = {https://doi.org/10.1080/00207548808947840},
  urldate = {2023-08-25},
  abstract = {Beam search is a technique for searching decision trees, particularly where the solution space is vast. The technique involves systematically developing a small number of solutions in parallel so as to attempt to maximize the probability of finding a good solution with minimal search effort. In this paper, we systematically study the performance behaviour of beam search with other heuristic methods for scheduling, and the effects of using different evaluation functions to guide the search. We also develop a new variation of beam search, called filtered beam search which is computationally simple yet produces high quality solutions.},
  file = {/home/pedro/Documents/doc/zotero/OW_MORTON_1988_Filtered beam search in scheduling†.pdf}
}

@book{papadimitriou1998combinatorial,
  title = {Combinatorial {{Optimization}}: {{Algorithms}} and {{Complexity}}},
  shorttitle = {Combinatorial {{Optimization}}},
  author = {Papadimitriou, Christos H. and Steiglitz, Kenneth},
  date = {1998-01-01},
  publisher = {{Courier Corporation}},
  abstract = {This clearly written, mathematically rigorous text includes a novel algorithmic exposition of the simplex method and also discusses the Soviet ellipsoid algorithm for linear programming; efficient algorithms for network flow, matching, spanning trees, and matroids; the theory of NP-complete problems; approximation algorithms, local search heuristics for NP-complete problems, more. All chapters are supplemented by thought-provoking problems. A useful work for graduate-level students with backgrounds in computer science, operations research, and electrical engineering. "Mathematicians wishing a self-contained introduction need look no further." — American Mathematical Monthly.},
  isbn = {978-0-486-40258-1},
  langid = {english},
  pagetotal = {530},
  keywords = {Mathematics / Combinatorics},
  file = {/home/pedro/Documents/doc/zotero/Papadimitriou_Steiglitz_1998_Combinatorial Optimization.pdf}
}

@incollection{resende2010greedya,
  title = {Greedy {{Randomized Adaptive Search Procedures}}: {{Advances}}, {{Hybridizations}}, and {{Applications}}},
  shorttitle = {Greedy {{Randomized Adaptive Search Procedures}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Resende, Mauricio G.C. and Ribeiro, Celso C.},
  editor = {Gendreau, Michel and Potvin, Jean-Yves},
  date = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {283--319},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-1665-5_10},
  url = {https://doi.org/10.1007/978-1-4419-1665-5_10},
  urldate = {2023-01-15},
  abstract = {GRASP is a multi-start metaheuristic for combinatorial optimization problems, in which each iteration consists basically of two phases: construction and local search. The construction phase builds a feasible solution, whose neighborhood is investigated until a local minimum is found during the local search phase. The best overall solution is kept as the result. In this chapter, we first describe the basic components of GRASP. Successful implementation techniques are discussed and illustrated by numerical results obtained for different applications. Enhanced or alternative solution construction mechanisms and techniques to speed up the search are also described: alternative randomized greedy construction schemes, Reactive GRASP, cost perturbations, bias functions, memory and learning, local search on partially constructed solutions, hashing, and filtering. We also discuss implementation strategies of memory-based intensification and post-optimization techniques using path-relinking. Hybridizations with other metaheuristics, parallelization strategies, and applications are also reviewed.},
  isbn = {978-1-4419-1665-5},
  langid = {english},
  keywords = {Bias Function,Construction Phase,Iterate Local Search,Local Search,Tabu Search},
  file = {/home/pedro/Documents/doc/zotero/Resende_Ribeiro_2010_Greedy Randomized Adaptive Search Procedures.pdf}
}

@article{rinnooykan1993class,
  title = {A Class of Generalized Greedy Algorithms for the Multi-Knapsack Problem},
  author = {Rinnooy Kan, A. H. G. and Stougie, L. and Vercellis, C.},
  date = {1993-04-27},
  journaltitle = {Discrete Applied Mathematics},
  shortjournal = {Discrete Applied Mathematics},
  volume = {42},
  number = {2},
  pages = {279--290},
  issn = {0166-218X},
  doi = {10.1016/0166-218X(93)90051-O},
  url = {https://www.sciencedirect.com/science/article/pii/0166218X9390051O},
  urldate = {2023-01-12},
  abstract = {A class of generalized greedy algorithms is proposed for the solution of the [lcub]0,1[rcub] multi-knapsack problem. Items are selected according to decreasing ratios of their profit and a weighted sum of their requirement coefficients. The solution obtained depends on the choice of the weights. A geometrical representation of the method is given and the relation to the dual of the linear programming relaxation of multi-knapsack is exploited. We investigate the complexity of computing a set of weights that gives the maximum greedy solution value. Finally, the heuristics are subjected to both a worst-case and a probabilistic performance analysis.},
  langid = {english},
  keywords = {computational complexity,greedy heuristic,linear programming relaxation,Multi-knapsack problem,probabilistic analysis,worst-case analysis},
  file = {/home/pedro/Documents/doc/zotero/Rinnooy Kan et al_1993_A class of generalized greedy algorithms for the multi-knapsack problem.pdf;/home/pedro/Zotero/storage/IDMBLAMX/0166218X9390051O.html}
}

@online{rodrigueshascodemodels,
  title = {Hascode-Models},
  author = {Rodrigues, Pedro},
  url = {https://github.com/pedromig/hashcode-models},
  urldate = {2023-08-31},
  langid = {english},
  organization = {{Github}},
  file = {/home/pedro/Zotero/storage/S2NA3CBC/hashcode-models.html}
}

@online{rodriguesnasf4niopy,
  title = {Nasf4nio-Py},
  author = {Rodrigues, Pedro},
  url = {https://github.com/pedromig/nasf4nio-py},
  urldate = {2023-08-31},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/pedro/Zotero/storage/KSQBP67T/nasf4nio-py.html}
}

@article{stutzle1999maxmin,
  title = {{{MAX-MIN}} Ant System},
  author = {Stützle, Thomas and Hoos, Holger},
  date = {1999-11-30},
  volume = {16},
  abstract = {Ant System, the first Ant Colony Optimization algorithm, showed to be a viable method for attacking hard combinatorial optimization problems. Yet, its performance, when compared to more fine-tuned algorithms, was rather poor for large instances of traditional benchmark problems like the Traveling Salesman Problem. To show that Ant Colony Optimization algorithms could be good alternatives to existing algorithms for hard combinatorial optimization problems, recent research in this ares has mainly focused on the development of algorithmic variants which achieve better performance than AS. In this article, we present MAX --MIN Ant System, an Ant Colony Optimization algorithm derived from Ant System. MAX --MIN Ant System differs from Ant System in several important aspects, whose usefulness we demonstrate by means of an experimental study. Additionally, we relate one of the characteristics specific to MMAS --- that of using a greedier search than Ant System --- to results from the search s...},
  file = {/home/pedro/Documents/doc/zotero/Stützle_Hoos_1999_MAX-MIN ant system.pdf}
}

@incollection{stutzle2018iterated,
  title = {Iterated {{Greedy}}},
  booktitle = {Handbook of {{Heuristics}}},
  author = {Stützle, Thomas and Ruiz, Rubén},
  editor = {Martí, Rafael and Pardalos, Panos M. and Resende, Mauricio G. C.},
  date = {2018},
  pages = {547--577},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-07124-4_10},
  url = {https://doi.org/10.1007/978-3-319-07124-4_10},
  urldate = {2023-08-25},
  abstract = {Iterated greedy is a search method that iterates through applications of construction heuristics using the repeated execution of two main phases, the partial destruction of a complete candidate solution and a subsequent reconstruction of a complete candidate solution. Iterated greedy is based on a simple principle, and methods based on this principle have been proposed and published several times in the literature under different names such as simulated annealing, iterative flattening, ruin-and-recreate, large neighborhood search, and others. Despite its simplicity, iterated greedy has led to rather high-performing algorithms. In combination with other heuristic optimization techniques such as a local search, it has given place to state-of-the-art algorithms for various problems. This paper reviews the main principles of iterated greedy algorithms, relates the basic technique to the various proposals based on this principle, discusses its relationship with other optimization techniques, and gives an overview of problems to which iterated greedy has been successfully applied.},
  isbn = {978-3-319-07124-4},
  langid = {english},
  keywords = {Constructive search,Greedy methods,Iterated greedy,Local search,Metaheuristics,Stochastic local search},
  file = {/home/pedro/Documents/doc/zotero/Stützle_Ruiz_2018_Iterated Greedy.pdf}
}

@thesis{vieira2009uma,
  type = {Doctoral Thesis},
  title = {Uma plataforma para a avaliação experimental de meta-heurísticas},
  author = {Vieira, Ana},
  date = {2009},
  institution = {{University of Algarve, Portugal}},
  url = {https://sapientia.ualg.pt/handle/10400.1/518},
  urldate = {2023-01-10},
  abstract = {Tese de dout., Engenharia Electrónica, Faculdade de Ciências e Tecnologia, Universidade do Algarve, 2009},
  langid = {portuguese},
  annotation = {Accepted: 2011-09-07T16:04:54Z},
  file = {/home/pedro/Documents/doc/zotero/Vieira_2009_Uma plataforma para a avaliação experimental de meta-heurísticas.pdf;/home/pedro/Zotero/storage/Q2ZTZGJP/518.html}
}

@book{williamson2011design,
  title = {The {{Design}} of {{Approximation Algorithms}}},
  author = {Williamson, David P. and Shmoys, David B.},
  date = {2011-04-26},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511921735},
  url = {https://www.cambridge.org/core/product/identifier/9780511921735/type/book},
  urldate = {2023-01-13},
  abstract = {Discrete optimization problems are everywhere, from traditional operations research planning (scheduling, facility location and network design); to computer science databases; to advertising issues in viral marketing. Yet most such problems are NP-hard; unless P = NP, there are no efficient algorithms to find optimal solutions. This book shows how to design approximation algorithms: efficient algorithms that find provably near-optimal solutions. The book is organized around central algorithmic techniques for designing approximation algorithms, including greedy and local search algorithms, dynamic programming, linear and semidefinite programming, and randomization. Each chapter in the first section is devoted to a single algorithmic technique applied to several different problems, with more sophisticated treatment in the second section. The book also covers methods for proving that optimization problems are hard to approximate. Designed as a textbook for graduate-level algorithm courses, it will also serve as a reference for researchers interested in the heuristic solution of discrete optimization problems.},
  isbn = {978-0-511-92173-5},
  langid = {english},
  file = {/home/pedro/Zotero/storage/4JX9SQJH/Williamson and Shmoys - 2011 - The Design of Approximation Algorithms.pdf}
}

@book{witelski2015methods,
  title = {Methods of {{Mathematical Modelling}}},
  author = {Witelski, Thomas and Bowen, Mark},
  date = {2015},
  series = {Springer {{Undergraduate Mathematics Series}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-23042-9},
  url = {http://link.springer.com/10.1007/978-3-319-23042-9},
  urldate = {2023-01-15},
  isbn = {978-3-319-23041-2},
  langid = {english},
  file = {/home/pedro/Zotero/storage/3HS7QHKI/Witelski and Bowen - 2015 - Methods of Mathematical Modelling.pdf}
}

@incollection{yu2010combinatorial,
  title = {Combinatorial {{Optimization}}},
  booktitle = {Introduction to {{Evolutionary Algorithms}}},
  author = {Yu, Xinjie and Gen, Mitsuo},
  editor = {Roy, Rajkumar},
  date = {2010},
  series = {Decision {{Engineering}}},
  pages = {263--324},
  publisher = {{Springer}},
  location = {{London}},
  doi = {10.1007/978-1-84996-129-5_7},
  url = {https://doi.org/10.1007/978-1-84996-129-5_7},
  urldate = {2023-01-12},
  abstract = {Previous chapters discuss parameter optimization, i.e., we need to find the optimal values of variables so that the objective function has the maximum/minimum value. Many real-world problems are not like this. We often need to select some elements from a set or arrange the sequence of some events with constraints so that the objective function has the maximum/minimum value. These problems belong to combinatorial optimization. We will introduce three examples, explain their respective properties, illustrate how EAs solve them, and summarize design-effective algorithms for them.},
  isbn = {978-1-84996-129-5},
  langid = {english},
  keywords = {Active Schedule,Hamiltonian Cycle,Knapsack Problem,Travel Salesman Problem},
  file = {/home/pedro/Documents/doc/zotero/2010_Combinatorial Optimization.pdf}
}

@online{zibada2023google,
  title = {Google {{Coding Competitions Unofficial Archive}}},
  author = {Zibada},
  date = {2023},
  url = {https://zibada.guru/gcj/#hc},
  urldate = {2023-08-20},
  file = {/home/pedro/Zotero/storage/3USQTLY4/gcj.html}
}
